{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPU machine FinalAssignment5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchitjain04/EIP/blob/master/Assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1dba2e0f-b6a7-4327-d27a-e8ba6bc0b424"
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!unzip -q \"/content/drive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aac5fe8d-2805-4ee3-d1b8-1440f11e2f3d"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16,resnet50,VGG19,Xception,MobileNetV2,InceptionV3,resnet_v2,inception_v3,inception_resnet_v2\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD,Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKpWFpju-Q5j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "905aeae2-a4c6-4ea3-fbfc-87f00cf80c43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "51ffd3e3-6023-4fdd-bccc-6bbad579dde6"
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYF4VUmjkWM1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26a649c4-5128-4f51-d719-38c617ec36d5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    def __init__(self, df, batch_size=64, shuffle=True, augmentation=True):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        \n",
        "        images = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])   \n",
        "\n",
        "        if self.augmentation is not None:\n",
        "            images = self.augmentation.flow(images, batch_size=64)\n",
        "        \n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        \n",
        "        return images, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_gen = PersonDataGenerator(\n",
        "    train_df, \n",
        "    batch_size=64, \n",
        "    augmentation=ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        featurewise_center=False,\n",
        "        featurewise_std_normalization=False,\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=.1,\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "cc717174-f24a-4252-d5d6-f8d99d5b0a82"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8938</th>\n",
              "      <td>resized/8939.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1800</th>\n",
              "      <td>resized/1801.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6499</th>\n",
              "      <td>resized/6500.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2217</th>\n",
              "      <td>resized/2218.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9104</th>\n",
              "      <td>resized/9105.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "8938  resized/8939.jpg              0  ...                        1              0\n",
              "1800  resized/1801.jpg              1  ...                        1              0\n",
              "6499  resized/6500.jpg              1  ...                        0              1\n",
              "2217  resized/2218.jpg              1  ...                        0              1\n",
              "9104  resized/9105.jpg              0  ...                        1              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=64)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "a5f0c377-da7d-478f-a5f6-e0ee1ef6afd2"
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-27bcab40e875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-5b8a95df21f8>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         target = {\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'flow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone =VGG19(\n",
        "    weights=None, \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.1)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.1)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.1)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.1)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxWVxcbi_y6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "opt = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=32, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebd2f15f-10df-4b84-9e71-aa69ad29d9a4"
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.8900 - gender_output_loss: 0.6887 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4413 - weight_output_loss: 0.9997 - bag_output_loss: 0.9343 - footwear_output_loss: 0.9736 - pose_output_loss: 0.9386 - emotion_output_loss: 0.9329 - gender_output_acc: 0.5506 - image_quality_output_acc: 0.5525 - age_output_acc: 0.3925 - weight_output_acc: 0.6377 - bag_output_acc: 0.5567 - footwear_output_acc: 0.5361 - pose_output_acc: 0.6127 - emotion_output_acc: 0.7118\n",
            "180/180 [==============================] - 100s 556ms/step - loss: 7.8878 - gender_output_loss: 0.6887 - image_quality_output_loss: 0.9813 - age_output_loss: 1.4407 - weight_output_loss: 0.9995 - bag_output_loss: 0.9342 - footwear_output_loss: 0.9733 - pose_output_loss: 0.9378 - emotion_output_loss: 0.9325 - gender_output_acc: 0.5507 - image_quality_output_acc: 0.5524 - age_output_acc: 0.3930 - weight_output_acc: 0.6379 - bag_output_acc: 0.5567 - footwear_output_acc: 0.5364 - pose_output_acc: 0.6134 - emotion_output_acc: 0.7119 - val_loss: 7.8274 - val_gender_output_loss: 0.6815 - val_image_quality_output_loss: 0.9954 - val_age_output_loss: 1.4323 - val_weight_output_loss: 1.0166 - val_bag_output_loss: 0.9222 - val_footwear_output_loss: 0.9287 - val_pose_output_loss: 0.9305 - val_emotion_output_loss: 0.9202 - val_gender_output_acc: 0.5691 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5726 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 2/100\n",
            "180/180 [==============================] - 91s 507ms/step - loss: 7.7553 - gender_output_loss: 0.6819 - image_quality_output_loss: 0.9706 - age_output_loss: 1.4274 - weight_output_loss: 0.9809 - bag_output_loss: 0.9193 - footwear_output_loss: 0.9405 - pose_output_loss: 0.9248 - emotion_output_loss: 0.9100 - gender_output_acc: 0.5607 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3971 - weight_output_acc: 0.6391 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5638 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7119 - val_loss: 7.7959 - val_gender_output_loss: 0.6731 - val_image_quality_output_loss: 0.9977 - val_age_output_loss: 1.4321 - val_weight_output_loss: 1.0067 - val_bag_output_loss: 0.9169 - val_footwear_output_loss: 0.9237 - val_pose_output_loss: 0.9286 - val_emotion_output_loss: 0.9173 - val_gender_output_acc: 0.5756 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5801 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 3/100\n",
            "180/180 [==============================] - 92s 508ms/step - loss: 7.7162 - gender_output_loss: 0.6799 - image_quality_output_loss: 0.9640 - age_output_loss: 1.4220 - weight_output_loss: 0.9769 - bag_output_loss: 0.9162 - footwear_output_loss: 0.9313 - pose_output_loss: 0.9192 - emotion_output_loss: 0.9067 - gender_output_acc: 0.5665 - image_quality_output_acc: 0.5575 - age_output_acc: 0.3980 - weight_output_acc: 0.6391 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5700 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7122 - val_loss: 7.7707 - val_gender_output_loss: 0.6736 - val_image_quality_output_loss: 1.0034 - val_age_output_loss: 1.4272 - val_weight_output_loss: 1.0028 - val_bag_output_loss: 0.9162 - val_footwear_output_loss: 0.9107 - val_pose_output_loss: 0.9218 - val_emotion_output_loss: 0.9150 - val_gender_output_acc: 0.5973 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5887 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 4/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6951 - gender_output_loss: 0.6758 - image_quality_output_loss: 0.9655 - age_output_loss: 1.4193 - weight_output_loss: 0.9741 - bag_output_loss: 0.9150 - footwear_output_loss: 0.9244 - pose_output_loss: 0.9166 - emotion_output_loss: 0.9044 - gender_output_acc: 0.5776 - image_quality_output_acc: 0.5568 - age_output_acc: 0.3983 - weight_output_acc: 0.6393 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5757 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7117 - val_loss: 7.8372 - val_gender_output_loss: 0.6908 - val_image_quality_output_loss: 0.9954 - val_age_output_loss: 1.4257 - val_weight_output_loss: 1.0029 - val_bag_output_loss: 0.9173 - val_footwear_output_loss: 0.9578 - val_pose_output_loss: 0.9323 - val_emotion_output_loss: 0.9151 - val_gender_output_acc: 0.5323 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5131 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 5/100\n",
            "180/180 [==============================] - 92s 508ms/step - loss: 7.6777 - gender_output_loss: 0.6734 - image_quality_output_loss: 0.9631 - age_output_loss: 1.4140 - weight_output_loss: 0.9726 - bag_output_loss: 0.9150 - footwear_output_loss: 0.9232 - pose_output_loss: 0.9136 - emotion_output_loss: 0.9028 - gender_output_acc: 0.5792 - image_quality_output_acc: 0.5564 - age_output_acc: 0.3979 - weight_output_acc: 0.6395 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5791 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7121 - val_loss: 7.7580 - val_gender_output_loss: 0.6844 - val_image_quality_output_loss: 0.9921 - val_age_output_loss: 1.4241 - val_weight_output_loss: 1.0037 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 0.9020 - val_pose_output_loss: 0.9176 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.5494 - val_image_quality_output_acc: 0.5272 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5963 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 6/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6588 - gender_output_loss: 0.6701 - image_quality_output_loss: 0.9607 - age_output_loss: 1.4126 - weight_output_loss: 0.9717 - bag_output_loss: 0.9137 - footwear_output_loss: 0.9151 - pose_output_loss: 0.9130 - emotion_output_loss: 0.9018 - gender_output_acc: 0.5812 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3980 - weight_output_acc: 0.6397 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5803 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7120 - val_loss: 7.7419 - val_gender_output_loss: 0.6657 - val_image_quality_output_loss: 0.9933 - val_age_output_loss: 1.4206 - val_weight_output_loss: 0.9992 - val_bag_output_loss: 0.9161 - val_footwear_output_loss: 0.9153 - val_pose_output_loss: 0.9198 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.5801 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5817 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 7/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6532 - gender_output_loss: 0.6714 - image_quality_output_loss: 0.9641 - age_output_loss: 1.4122 - weight_output_loss: 0.9692 - bag_output_loss: 0.9110 - footwear_output_loss: 0.9124 - pose_output_loss: 0.9124 - emotion_output_loss: 0.9006 - gender_output_acc: 0.5808 - image_quality_output_acc: 0.5575 - age_output_acc: 0.3984 - weight_output_acc: 0.6397 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5823 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7123 - val_loss: 7.6973 - val_gender_output_loss: 0.6592 - val_image_quality_output_loss: 1.0020 - val_age_output_loss: 1.4146 - val_weight_output_loss: 0.9968 - val_bag_output_loss: 0.9163 - val_footwear_output_loss: 0.8793 - val_pose_output_loss: 0.9169 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.5887 - val_image_quality_output_acc: 0.5247 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6048 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 8/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6405 - gender_output_loss: 0.6685 - image_quality_output_loss: 0.9614 - age_output_loss: 1.4089 - weight_output_loss: 0.9713 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9047 - pose_output_loss: 0.9116 - emotion_output_loss: 0.9018 - gender_output_acc: 0.5816 - image_quality_output_acc: 0.5560 - age_output_acc: 0.3990 - weight_output_acc: 0.6392 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5858 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7122 - val_loss: 7.7535 - val_gender_output_loss: 0.6531 - val_image_quality_output_loss: 0.9949 - val_age_output_loss: 1.4180 - val_weight_output_loss: 0.9992 - val_bag_output_loss: 0.9190 - val_footwear_output_loss: 0.9363 - val_pose_output_loss: 0.9127 - val_emotion_output_loss: 0.9205 - val_gender_output_acc: 0.6094 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5660 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 9/100\n",
            "180/180 [==============================] - 92s 511ms/step - loss: 7.6275 - gender_output_loss: 0.6642 - image_quality_output_loss: 0.9616 - age_output_loss: 1.4060 - weight_output_loss: 0.9706 - bag_output_loss: 0.9114 - footwear_output_loss: 0.9014 - pose_output_loss: 0.9115 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5858 - image_quality_output_acc: 0.5581 - age_output_acc: 0.3982 - weight_output_acc: 0.6393 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5872 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7119 - val_loss: 7.6832 - val_gender_output_loss: 0.6480 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.4122 - val_weight_output_loss: 1.0008 - val_bag_output_loss: 0.9150 - val_footwear_output_loss: 0.8816 - val_pose_output_loss: 0.9136 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.6139 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6089 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 10/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6134 - gender_output_loss: 0.6619 - image_quality_output_loss: 0.9610 - age_output_loss: 1.4061 - weight_output_loss: 0.9686 - bag_output_loss: 0.9129 - footwear_output_loss: 0.8974 - pose_output_loss: 0.9053 - emotion_output_loss: 0.9002 - gender_output_acc: 0.5953 - image_quality_output_acc: 0.5575 - age_output_acc: 0.3961 - weight_output_acc: 0.6397 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5919 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7121 - val_loss: 7.7057 - val_gender_output_loss: 0.6576 - val_image_quality_output_loss: 0.9938 - val_age_output_loss: 1.4105 - val_weight_output_loss: 0.9960 - val_bag_output_loss: 0.9135 - val_footwear_output_loss: 0.9086 - val_pose_output_loss: 0.9130 - val_emotion_output_loss: 0.9128 - val_gender_output_acc: 0.6139 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5912 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 11/100\n",
            "180/180 [==============================] - 91s 507ms/step - loss: 7.6224 - gender_output_loss: 0.6593 - image_quality_output_loss: 0.9592 - age_output_loss: 1.4091 - weight_output_loss: 0.9687 - bag_output_loss: 0.9104 - footwear_output_loss: 0.9013 - pose_output_loss: 0.9139 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5928 - image_quality_output_acc: 0.5563 - age_output_acc: 0.3972 - weight_output_acc: 0.6392 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5900 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7121 - val_loss: 7.6625 - val_gender_output_loss: 0.6439 - val_image_quality_output_loss: 0.9920 - val_age_output_loss: 1.4084 - val_weight_output_loss: 1.0022 - val_bag_output_loss: 0.9149 - val_footwear_output_loss: 0.8715 - val_pose_output_loss: 0.9178 - val_emotion_output_loss: 0.9118 - val_gender_output_acc: 0.6159 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6064 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 12/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6224 - gender_output_loss: 0.6549 - image_quality_output_loss: 0.9625 - age_output_loss: 1.4081 - weight_output_loss: 0.9686 - bag_output_loss: 0.9106 - footwear_output_loss: 0.9031 - pose_output_loss: 0.9137 - emotion_output_loss: 0.9010 - gender_output_acc: 0.5953 - image_quality_output_acc: 0.5574 - age_output_acc: 0.3968 - weight_output_acc: 0.6391 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5860 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7118 - val_loss: 7.7326 - val_gender_output_loss: 0.6765 - val_image_quality_output_loss: 0.9943 - val_age_output_loss: 1.4167 - val_weight_output_loss: 0.9986 - val_bag_output_loss: 0.9163 - val_footwear_output_loss: 0.8945 - val_pose_output_loss: 0.9219 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.5640 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5736 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 13/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6386 - gender_output_loss: 0.6638 - image_quality_output_loss: 0.9620 - age_output_loss: 1.4097 - weight_output_loss: 0.9702 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9050 - pose_output_loss: 0.9144 - emotion_output_loss: 0.9013 - gender_output_acc: 0.5873 - image_quality_output_acc: 0.5582 - age_output_acc: 0.3984 - weight_output_acc: 0.6393 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5854 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7121 - val_loss: 7.7342 - val_gender_output_loss: 0.6809 - val_image_quality_output_loss: 1.0035 - val_age_output_loss: 1.4158 - val_weight_output_loss: 0.9979 - val_bag_output_loss: 0.9143 - val_footwear_output_loss: 0.8872 - val_pose_output_loss: 0.9236 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.5811 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4037 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6033 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 14/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6294 - gender_output_loss: 0.6620 - image_quality_output_loss: 0.9628 - age_output_loss: 1.4062 - weight_output_loss: 0.9686 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9052 - pose_output_loss: 0.9127 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5984 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3982 - weight_output_acc: 0.6391 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5884 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7122 - val_loss: 7.6910 - val_gender_output_loss: 0.6526 - val_image_quality_output_loss: 1.0044 - val_age_output_loss: 1.4100 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.9160 - val_footwear_output_loss: 0.8860 - val_pose_output_loss: 0.9170 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.5932 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4088 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6028 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 15/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6520 - gender_output_loss: 0.6596 - image_quality_output_loss: 0.9615 - age_output_loss: 1.4093 - weight_output_loss: 0.9707 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9238 - pose_output_loss: 0.9144 - emotion_output_loss: 0.9004 - gender_output_acc: 0.5952 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3970 - weight_output_acc: 0.6392 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5690 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7124 - val_loss: 7.7462 - val_gender_output_loss: 0.6753 - val_image_quality_output_loss: 0.9924 - val_age_output_loss: 1.4218 - val_weight_output_loss: 1.0005 - val_bag_output_loss: 0.9172 - val_footwear_output_loss: 0.9086 - val_pose_output_loss: 0.9180 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.5801 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5958 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 16/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6649 - gender_output_loss: 0.6745 - image_quality_output_loss: 0.9632 - age_output_loss: 1.4130 - weight_output_loss: 0.9696 - bag_output_loss: 0.9146 - footwear_output_loss: 0.9176 - pose_output_loss: 0.9113 - emotion_output_loss: 0.9011 - gender_output_acc: 0.5765 - image_quality_output_acc: 0.5588 - age_output_acc: 0.3976 - weight_output_acc: 0.6391 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5831 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7124 - val_loss: 7.7118 - val_gender_output_loss: 0.6577 - val_image_quality_output_loss: 0.9950 - val_age_output_loss: 1.4125 - val_weight_output_loss: 0.9971 - val_bag_output_loss: 0.9178 - val_footwear_output_loss: 0.9069 - val_pose_output_loss: 0.9126 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.5907 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5948 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 17/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6242 - gender_output_loss: 0.6658 - image_quality_output_loss: 0.9611 - age_output_loss: 1.4085 - weight_output_loss: 0.9694 - bag_output_loss: 0.9134 - footwear_output_loss: 0.8993 - pose_output_loss: 0.9066 - emotion_output_loss: 0.9000 - gender_output_acc: 0.5918 - image_quality_output_acc: 0.5592 - age_output_acc: 0.3984 - weight_output_acc: 0.6396 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5926 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7122 - val_loss: 7.7452 - val_gender_output_loss: 0.6734 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.4200 - val_weight_output_loss: 1.0002 - val_bag_output_loss: 0.9171 - val_footwear_output_loss: 0.9036 - val_pose_output_loss: 0.9202 - val_emotion_output_loss: 0.9148 - val_gender_output_acc: 0.5902 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5806 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 18/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6839 - gender_output_loss: 0.6815 - image_quality_output_loss: 0.9663 - age_output_loss: 1.4173 - weight_output_loss: 0.9704 - bag_output_loss: 0.9135 - footwear_output_loss: 0.9191 - pose_output_loss: 0.9155 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5701 - image_quality_output_acc: 0.5571 - age_output_acc: 0.3977 - weight_output_acc: 0.6392 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5777 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7119 - val_loss: 7.7529 - val_gender_output_loss: 0.6761 - val_image_quality_output_loss: 1.0043 - val_age_output_loss: 1.4245 - val_weight_output_loss: 1.0002 - val_bag_output_loss: 0.9162 - val_footwear_output_loss: 0.8928 - val_pose_output_loss: 0.9221 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.5761 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6008 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 19/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6624 - gender_output_loss: 0.6791 - image_quality_output_loss: 0.9654 - age_output_loss: 1.4146 - weight_output_loss: 0.9710 - bag_output_loss: 0.9123 - footwear_output_loss: 0.9060 - pose_output_loss: 0.9124 - emotion_output_loss: 0.9013 - gender_output_acc: 0.5722 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3978 - weight_output_acc: 0.6394 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5845 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7122 - val_loss: 7.7312 - val_gender_output_loss: 0.6680 - val_image_quality_output_loss: 1.0043 - val_age_output_loss: 1.4178 - val_weight_output_loss: 0.9982 - val_bag_output_loss: 0.9165 - val_footwear_output_loss: 0.8973 - val_pose_output_loss: 0.9148 - val_emotion_output_loss: 0.9142 - val_gender_output_acc: 0.5938 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5948 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 20/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6580 - gender_output_loss: 0.6754 - image_quality_output_loss: 0.9649 - age_output_loss: 1.4127 - weight_output_loss: 0.9703 - bag_output_loss: 0.9125 - footwear_output_loss: 0.9102 - pose_output_loss: 0.9098 - emotion_output_loss: 0.9022 - gender_output_acc: 0.5774 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3984 - weight_output_acc: 0.6390 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5828 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7123 - val_loss: 7.7799 - val_gender_output_loss: 0.6745 - val_image_quality_output_loss: 1.0048 - val_age_output_loss: 1.4369 - val_weight_output_loss: 0.9964 - val_bag_output_loss: 0.9151 - val_footwear_output_loss: 0.9167 - val_pose_output_loss: 0.9172 - val_emotion_output_loss: 0.9183 - val_gender_output_acc: 0.5862 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5731 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 21/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6941 - gender_output_loss: 0.6740 - image_quality_output_loss: 0.9686 - age_output_loss: 1.4150 - weight_output_loss: 0.9713 - bag_output_loss: 0.9129 - footwear_output_loss: 0.9347 - pose_output_loss: 0.9143 - emotion_output_loss: 0.9033 - gender_output_acc: 0.5794 - image_quality_output_acc: 0.5587 - age_output_acc: 0.3970 - weight_output_acc: 0.6395 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5621 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7118 - val_loss: 7.7308 - val_gender_output_loss: 0.6634 - val_image_quality_output_loss: 0.9944 - val_age_output_loss: 1.4165 - val_weight_output_loss: 0.9976 - val_bag_output_loss: 0.9150 - val_footwear_output_loss: 0.9040 - val_pose_output_loss: 0.9258 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.5968 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5852 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 22/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.7017 - gender_output_loss: 0.6803 - image_quality_output_loss: 0.9692 - age_output_loss: 1.4173 - weight_output_loss: 0.9730 - bag_output_loss: 0.9136 - footwear_output_loss: 0.9280 - pose_output_loss: 0.9183 - emotion_output_loss: 0.9021 - gender_output_acc: 0.5681 - image_quality_output_acc: 0.5580 - age_output_acc: 0.3970 - weight_output_acc: 0.6395 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5691 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7121 - val_loss: 7.7421 - val_gender_output_loss: 0.6609 - val_image_quality_output_loss: 0.9973 - val_age_output_loss: 1.4154 - val_weight_output_loss: 0.9999 - val_bag_output_loss: 0.9151 - val_footwear_output_loss: 0.9188 - val_pose_output_loss: 0.9210 - val_emotion_output_loss: 0.9136 - val_gender_output_acc: 0.5927 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5801 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 23/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6792 - gender_output_loss: 0.6740 - image_quality_output_loss: 0.9669 - age_output_loss: 1.4138 - weight_output_loss: 0.9707 - bag_output_loss: 0.9124 - footwear_output_loss: 0.9232 - pose_output_loss: 0.9159 - emotion_output_loss: 0.9025 - gender_output_acc: 0.5794 - image_quality_output_acc: 0.5582 - age_output_acc: 0.3965 - weight_output_acc: 0.6393 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5735 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7118 - val_loss: 7.7948 - val_gender_output_loss: 0.6738 - val_image_quality_output_loss: 1.0001 - val_age_output_loss: 1.4187 - val_weight_output_loss: 1.0010 - val_bag_output_loss: 0.9168 - val_footwear_output_loss: 0.9418 - val_pose_output_loss: 0.9285 - val_emotion_output_loss: 0.9142 - val_gender_output_acc: 0.5806 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5580 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 24/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6610 - gender_output_loss: 0.6656 - image_quality_output_loss: 0.9665 - age_output_loss: 1.4079 - weight_output_loss: 0.9698 - bag_output_loss: 0.9120 - footwear_output_loss: 0.9225 - pose_output_loss: 0.9147 - emotion_output_loss: 0.9019 - gender_output_acc: 0.5948 - image_quality_output_acc: 0.5572 - age_output_acc: 0.3984 - weight_output_acc: 0.6394 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5751 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7119 - val_loss: 7.6970 - val_gender_output_loss: 0.6497 - val_image_quality_output_loss: 0.9959 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9956 - val_bag_output_loss: 0.9130 - val_footwear_output_loss: 0.9056 - val_pose_output_loss: 0.9165 - val_emotion_output_loss: 0.9123 - val_gender_output_acc: 0.5988 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5796 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 25/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6358 - gender_output_loss: 0.6618 - image_quality_output_loss: 0.9639 - age_output_loss: 1.4083 - weight_output_loss: 0.9705 - bag_output_loss: 0.9108 - footwear_output_loss: 0.9094 - pose_output_loss: 0.9104 - emotion_output_loss: 0.9006 - gender_output_acc: 0.6041 - image_quality_output_acc: 0.5582 - age_output_acc: 0.3977 - weight_output_acc: 0.6391 - bag_output_acc: 0.5631 - footwear_output_acc: 0.5816 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7120 - val_loss: 7.7081 - val_gender_output_loss: 0.6529 - val_image_quality_output_loss: 0.9972 - val_age_output_loss: 1.4142 - val_weight_output_loss: 0.9981 - val_bag_output_loss: 0.9161 - val_footwear_output_loss: 0.8991 - val_pose_output_loss: 0.9140 - val_emotion_output_loss: 0.9164 - val_gender_output_acc: 0.6109 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5993 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 26/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6700 - gender_output_loss: 0.6751 - image_quality_output_loss: 0.9662 - age_output_loss: 1.4152 - weight_output_loss: 0.9716 - bag_output_loss: 0.9118 - footwear_output_loss: 0.9114 - pose_output_loss: 0.9171 - emotion_output_loss: 0.9016 - gender_output_acc: 0.5714 - image_quality_output_acc: 0.5580 - age_output_acc: 0.3971 - weight_output_acc: 0.6392 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5849 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7119 - val_loss: 7.7305 - val_gender_output_loss: 0.6683 - val_image_quality_output_loss: 1.0034 - val_age_output_loss: 1.4179 - val_weight_output_loss: 0.9984 - val_bag_output_loss: 0.9150 - val_footwear_output_loss: 0.8881 - val_pose_output_loss: 0.9235 - val_emotion_output_loss: 0.9159 - val_gender_output_acc: 0.5897 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6008 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 27/100\n",
            "180/180 [==============================] - 92s 511ms/step - loss: 7.6531 - gender_output_loss: 0.6769 - image_quality_output_loss: 0.9655 - age_output_loss: 1.4100 - weight_output_loss: 0.9713 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9026 - pose_output_loss: 0.9158 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5666 - image_quality_output_acc: 0.5573 - age_output_acc: 0.3986 - weight_output_acc: 0.6392 - bag_output_acc: 0.5639 - footwear_output_acc: 0.5892 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7121 - val_loss: 7.7173 - val_gender_output_loss: 0.6647 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.4144 - val_weight_output_loss: 0.9978 - val_bag_output_loss: 0.9168 - val_footwear_output_loss: 0.8916 - val_pose_output_loss: 0.9201 - val_emotion_output_loss: 0.9138 - val_gender_output_acc: 0.5917 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5973 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 28/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6449 - gender_output_loss: 0.6718 - image_quality_output_loss: 0.9673 - age_output_loss: 1.4116 - weight_output_loss: 0.9697 - bag_output_loss: 0.9119 - footwear_output_loss: 0.8992 - pose_output_loss: 0.9127 - emotion_output_loss: 0.9006 - gender_output_acc: 0.5826 - image_quality_output_acc: 0.5567 - age_output_acc: 0.3972 - weight_output_acc: 0.6394 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5919 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7122 - val_loss: 7.6918 - val_gender_output_loss: 0.6575 - val_image_quality_output_loss: 0.9983 - val_age_output_loss: 1.4130 - val_weight_output_loss: 0.9950 - val_bag_output_loss: 0.9160 - val_footwear_output_loss: 0.8762 - val_pose_output_loss: 0.9191 - val_emotion_output_loss: 0.9166 - val_gender_output_acc: 0.6033 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5983 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 29/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6656 - gender_output_loss: 0.6738 - image_quality_output_loss: 0.9651 - age_output_loss: 1.4138 - weight_output_loss: 0.9702 - bag_output_loss: 0.9119 - footwear_output_loss: 0.9107 - pose_output_loss: 0.9179 - emotion_output_loss: 0.9022 - gender_output_acc: 0.5773 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3977 - weight_output_acc: 0.6390 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5850 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7122 - val_loss: 7.7716 - val_gender_output_loss: 0.6875 - val_image_quality_output_loss: 0.9969 - val_age_output_loss: 1.4315 - val_weight_output_loss: 1.0087 - val_bag_output_loss: 0.9165 - val_footwear_output_loss: 0.8883 - val_pose_output_loss: 0.9236 - val_emotion_output_loss: 0.9185 - val_gender_output_acc: 0.5413 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5998 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 30/100\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.6291 - gender_output_loss: 0.6683 - image_quality_output_loss: 0.9658 - age_output_loss: 1.4089 - weight_output_loss: 0.9695 - bag_output_loss: 0.9116 - footwear_output_loss: 0.8944 - pose_output_loss: 0.9112 - emotion_output_loss: 0.8994 - gender_output_acc: 0.5854 - image_quality_output_acc: 0.5567 - age_output_acc: 0.3989 - weight_output_acc: 0.6394 - bag_output_acc: 0.5643 - footwear_output_acc: 0.5932 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7119Epoch 30/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6281 - gender_output_loss: 0.6681 - image_quality_output_loss: 0.9664 - age_output_loss: 1.4085 - weight_output_loss: 0.9699 - bag_output_loss: 0.9120 - footwear_output_loss: 0.8938 - pose_output_loss: 0.9105 - emotion_output_loss: 0.8989 - gender_output_acc: 0.5860 - image_quality_output_acc: 0.5560 - age_output_acc: 0.3994 - weight_output_acc: 0.6394 - bag_output_acc: 0.5641 - footwear_output_acc: 0.5937 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7121 - val_loss: 7.6994 - val_gender_output_loss: 0.6630 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.4122 - val_weight_output_loss: 0.9982 - val_bag_output_loss: 0.9155 - val_footwear_output_loss: 0.8809 - val_pose_output_loss: 0.9154 - val_emotion_output_loss: 0.9183 - val_gender_output_acc: 0.5948 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5827 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 31/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6291 - gender_output_loss: 0.6649 - image_quality_output_loss: 0.9646 - age_output_loss: 1.4091 - weight_output_loss: 0.9692 - bag_output_loss: 0.9114 - footwear_output_loss: 0.8986 - pose_output_loss: 0.9107 - emotion_output_loss: 0.9005 - gender_output_acc: 0.5941 - image_quality_output_acc: 0.5587 - age_output_acc: 0.3988 - weight_output_acc: 0.6391 - bag_output_acc: 0.5638 - footwear_output_acc: 0.5922 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7122 - val_loss: 7.6738 - val_gender_output_loss: 0.6469 - val_image_quality_output_loss: 0.9937 - val_age_output_loss: 1.4053 - val_weight_output_loss: 1.0032 - val_bag_output_loss: 0.9137 - val_footwear_output_loss: 0.8855 - val_pose_output_loss: 0.9122 - val_emotion_output_loss: 0.9131 - val_gender_output_acc: 0.6280 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5983 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 32/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6220 - gender_output_loss: 0.6589 - image_quality_output_loss: 0.9626 - age_output_loss: 1.4063 - weight_output_loss: 0.9688 - bag_output_loss: 0.9112 - footwear_output_loss: 0.9059 - pose_output_loss: 0.9076 - emotion_output_loss: 0.9007 - gender_output_acc: 0.6036 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3984 - weight_output_acc: 0.6392 - bag_output_acc: 0.5639 - footwear_output_acc: 0.5876 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7120 - val_loss: 7.6459 - val_gender_output_loss: 0.6405 - val_image_quality_output_loss: 1.0000 - val_age_output_loss: 1.4037 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.9140 - val_footwear_output_loss: 0.8725 - val_pose_output_loss: 0.9096 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.6053 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.4073 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6048 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 33/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6620 - gender_output_loss: 0.6698 - image_quality_output_loss: 0.9650 - age_output_loss: 1.4093 - weight_output_loss: 0.9688 - bag_output_loss: 0.9116 - footwear_output_loss: 0.9231 - pose_output_loss: 0.9127 - emotion_output_loss: 0.9017 - gender_output_acc: 0.5799 - image_quality_output_acc: 0.5579 - age_output_acc: 0.3988 - weight_output_acc: 0.6396 - bag_output_acc: 0.5641 - footwear_output_acc: 0.5722 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7121 - val_loss: 7.7488 - val_gender_output_loss: 0.6672 - val_image_quality_output_loss: 0.9991 - val_age_output_loss: 1.4131 - val_weight_output_loss: 0.9957 - val_bag_output_loss: 0.9134 - val_footwear_output_loss: 0.9153 - val_pose_output_loss: 0.9315 - val_emotion_output_loss: 0.9134 - val_gender_output_acc: 0.5660 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5736 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 34/100\n",
            "180/180 [==============================] - 92s 511ms/step - loss: 7.6415 - gender_output_loss: 0.6694 - image_quality_output_loss: 0.9647 - age_output_loss: 1.4065 - weight_output_loss: 0.9678 - bag_output_loss: 0.9113 - footwear_output_loss: 0.9098 - pose_output_loss: 0.9114 - emotion_output_loss: 0.9006 - gender_output_acc: 0.5779 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3990 - weight_output_acc: 0.6391 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5763 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7122 - val_loss: 7.7245 - val_gender_output_loss: 0.6740 - val_image_quality_output_loss: 1.0008 - val_age_output_loss: 1.4180 - val_weight_output_loss: 1.0015 - val_bag_output_loss: 0.9163 - val_footwear_output_loss: 0.8822 - val_pose_output_loss: 0.9200 - val_emotion_output_loss: 0.9117 - val_gender_output_acc: 0.5736 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6003 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 35/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6242 - gender_output_loss: 0.6642 - image_quality_output_loss: 0.9624 - age_output_loss: 1.4071 - weight_output_loss: 0.9696 - bag_output_loss: 0.9090 - footwear_output_loss: 0.9033 - pose_output_loss: 0.9088 - emotion_output_loss: 0.8999 - gender_output_acc: 0.5867 - image_quality_output_acc: 0.5586 - age_output_acc: 0.3982 - weight_output_acc: 0.6393 - bag_output_acc: 0.5641 - footwear_output_acc: 0.5839 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7120 - val_loss: 7.6698 - val_gender_output_loss: 0.6557 - val_image_quality_output_loss: 0.9980 - val_age_output_loss: 1.4116 - val_weight_output_loss: 0.9951 - val_bag_output_loss: 0.9135 - val_footwear_output_loss: 0.8713 - val_pose_output_loss: 0.9110 - val_emotion_output_loss: 0.9135 - val_gender_output_acc: 0.6250 - val_image_quality_output_acc: 0.5282 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6144 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 36/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6102 - gender_output_loss: 0.6581 - image_quality_output_loss: 0.9618 - age_output_loss: 1.4047 - weight_output_loss: 0.9694 - bag_output_loss: 0.9098 - footwear_output_loss: 0.9015 - pose_output_loss: 0.9053 - emotion_output_loss: 0.8995 - gender_output_acc: 0.6001 - image_quality_output_acc: 0.5575 - age_output_acc: 0.3978 - weight_output_acc: 0.6390 - bag_output_acc: 0.5651 - footwear_output_acc: 0.5858 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7121 - val_loss: 7.6470 - val_gender_output_loss: 0.6394 - val_image_quality_output_loss: 0.9947 - val_age_output_loss: 1.4039 - val_weight_output_loss: 0.9952 - val_bag_output_loss: 0.9106 - val_footwear_output_loss: 0.8795 - val_pose_output_loss: 0.9121 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.6184 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5685 - val_footwear_output_acc: 0.6058 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 37/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.5909 - gender_output_loss: 0.6484 - image_quality_output_loss: 0.9597 - age_output_loss: 1.4019 - weight_output_loss: 0.9654 - bag_output_loss: 0.9081 - footwear_output_loss: 0.9036 - pose_output_loss: 0.9061 - emotion_output_loss: 0.8978 - gender_output_acc: 0.6069 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3953 - weight_output_acc: 0.6398 - bag_output_acc: 0.5633 - footwear_output_acc: 0.5833 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7123 - val_loss: 7.6611 - val_gender_output_loss: 0.6339 - val_image_quality_output_loss: 0.9984 - val_age_output_loss: 1.4054 - val_weight_output_loss: 0.9930 - val_bag_output_loss: 0.9100 - val_footwear_output_loss: 0.8947 - val_pose_output_loss: 0.9116 - val_emotion_output_loss: 0.9140 - val_gender_output_acc: 0.6275 - val_image_quality_output_acc: 0.5267 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5721 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 38/100\n",
            "180/180 [==============================] - 91s 507ms/step - loss: 7.5719 - gender_output_loss: 0.6407 - image_quality_output_loss: 0.9602 - age_output_loss: 1.4006 - weight_output_loss: 0.9641 - bag_output_loss: 0.9071 - footwear_output_loss: 0.8969 - pose_output_loss: 0.9039 - emotion_output_loss: 0.8984 - gender_output_acc: 0.6210 - image_quality_output_acc: 0.5581 - age_output_acc: 0.3978 - weight_output_acc: 0.6394 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5922 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7119 - val_loss: 7.6539 - val_gender_output_loss: 0.6327 - val_image_quality_output_loss: 0.9962 - val_age_output_loss: 1.4016 - val_weight_output_loss: 0.9966 - val_bag_output_loss: 0.9092 - val_footwear_output_loss: 0.9006 - val_pose_output_loss: 0.9034 - val_emotion_output_loss: 0.9137 - val_gender_output_acc: 0.6411 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5776 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 39/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.5934 - gender_output_loss: 0.6489 - image_quality_output_loss: 0.9635 - age_output_loss: 1.4042 - weight_output_loss: 0.9673 - bag_output_loss: 0.9080 - footwear_output_loss: 0.8986 - pose_output_loss: 0.9051 - emotion_output_loss: 0.8978 - gender_output_acc: 0.6168 - image_quality_output_acc: 0.5570 - age_output_acc: 0.3976 - weight_output_acc: 0.6392 - bag_output_acc: 0.5632 - footwear_output_acc: 0.5884 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7120 - val_loss: 7.7697 - val_gender_output_loss: 0.6652 - val_image_quality_output_loss: 0.9947 - val_age_output_loss: 1.4161 - val_weight_output_loss: 1.0010 - val_bag_output_loss: 0.9148 - val_footwear_output_loss: 0.9530 - val_pose_output_loss: 0.9126 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.5958 - val_image_quality_output_acc: 0.5257 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5277 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 40/100\n",
            "179/180 [============================>.] - ETA: 0s - loss: 7.5850 - gender_output_loss: 0.6505 - image_quality_output_loss: 0.9596 - age_output_loss: 1.3989 - weight_output_loss: 0.9663 - bag_output_loss: 0.9085 - footwear_output_loss: 0.8972 - pose_output_loss: 0.9050 - emotion_output_loss: 0.8992 - gender_output_acc: 0.6108 - image_quality_output_acc: 0.5573 - age_output_acc: 0.3982 - weight_output_acc: 0.6397 - bag_output_acc: 0.5629 - footwear_output_acc: 0.5871 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7118Epoch 40/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.5851 - gender_output_loss: 0.6506 - image_quality_output_loss: 0.9596 - age_output_loss: 1.3993 - weight_output_loss: 0.9668 - bag_output_loss: 0.9078 - footwear_output_loss: 0.8971 - pose_output_loss: 0.9049 - emotion_output_loss: 0.8988 - gender_output_acc: 0.6109 - image_quality_output_acc: 0.5574 - age_output_acc: 0.3977 - weight_output_acc: 0.6395 - bag_output_acc: 0.5634 - footwear_output_acc: 0.5870 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7120 - val_loss: 7.6871 - val_gender_output_loss: 0.6528 - val_image_quality_output_loss: 0.9924 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9942 - val_bag_output_loss: 0.9118 - val_footwear_output_loss: 0.9021 - val_pose_output_loss: 0.9124 - val_emotion_output_loss: 0.9130 - val_gender_output_acc: 0.6028 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5811 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 41/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6443 - gender_output_loss: 0.6613 - image_quality_output_loss: 0.9601 - age_output_loss: 1.4038 - weight_output_loss: 0.9684 - bag_output_loss: 0.9095 - footwear_output_loss: 0.9319 - pose_output_loss: 0.9083 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5865 - image_quality_output_acc: 0.5589 - age_output_acc: 0.3984 - weight_output_acc: 0.6392 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5595 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7119 - val_loss: 7.6944 - val_gender_output_loss: 0.6547 - val_image_quality_output_loss: 1.0012 - val_age_output_loss: 1.4090 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.9101 - val_footwear_output_loss: 0.9032 - val_pose_output_loss: 0.9100 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.6033 - val_image_quality_output_acc: 0.5282 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5872 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 42/100\n",
            "180/180 [==============================] - 91s 507ms/step - loss: 7.6055 - gender_output_loss: 0.6525 - image_quality_output_loss: 0.9612 - age_output_loss: 1.4006 - weight_output_loss: 0.9675 - bag_output_loss: 0.9054 - footwear_output_loss: 0.9151 - pose_output_loss: 0.9040 - emotion_output_loss: 0.8992 - gender_output_acc: 0.6006 - image_quality_output_acc: 0.5604 - age_output_acc: 0.3964 - weight_output_acc: 0.6391 - bag_output_acc: 0.5644 - footwear_output_acc: 0.5797 - pose_output_acc: 0.6183 - emotion_output_acc: 0.7119 - val_loss: 7.6784 - val_gender_output_loss: 0.6694 - val_image_quality_output_loss: 0.9970 - val_age_output_loss: 1.4049 - val_weight_output_loss: 0.9949 - val_bag_output_loss: 0.9151 - val_footwear_output_loss: 0.8806 - val_pose_output_loss: 0.9072 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.5862 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5932 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 43/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5801 - gender_output_loss: 0.6476 - image_quality_output_loss: 0.9589 - age_output_loss: 1.3986 - weight_output_loss: 0.9653 - bag_output_loss: 0.9041 - footwear_output_loss: 0.9071 - pose_output_loss: 0.8995 - emotion_output_loss: 0.8990 - gender_output_acc: 0.6081 - image_quality_output_acc: 0.5577 - age_output_acc: 0.3990 - weight_output_acc: 0.6392 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5785 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7119 - val_loss: 7.6374 - val_gender_output_loss: 0.6384 - val_image_quality_output_loss: 0.9983 - val_age_output_loss: 1.4004 - val_weight_output_loss: 0.9924 - val_bag_output_loss: 0.9090 - val_footwear_output_loss: 0.8793 - val_pose_output_loss: 0.9103 - val_emotion_output_loss: 0.9093 - val_gender_output_acc: 0.6280 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5696 - val_footwear_output_acc: 0.6003 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 44/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.5983 - gender_output_loss: 0.6486 - image_quality_output_loss: 0.9629 - age_output_loss: 1.4021 - weight_output_loss: 0.9685 - bag_output_loss: 0.9060 - footwear_output_loss: 0.9072 - pose_output_loss: 0.9033 - emotion_output_loss: 0.8995 - gender_output_acc: 0.6076 - image_quality_output_acc: 0.5580 - age_output_acc: 0.3981 - weight_output_acc: 0.6396 - bag_output_acc: 0.5644 - footwear_output_acc: 0.5819 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7122 - val_loss: 7.6168 - val_gender_output_loss: 0.6363 - val_image_quality_output_loss: 1.0001 - val_age_output_loss: 1.3979 - val_weight_output_loss: 0.9923 - val_bag_output_loss: 0.9078 - val_footwear_output_loss: 0.8734 - val_pose_output_loss: 0.9006 - val_emotion_output_loss: 0.9084 - val_gender_output_acc: 0.6260 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5675 - val_footwear_output_acc: 0.6038 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 45/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5690 - gender_output_loss: 0.6438 - image_quality_output_loss: 0.9595 - age_output_loss: 1.3976 - weight_output_loss: 0.9659 - bag_output_loss: 0.9069 - footwear_output_loss: 0.8997 - pose_output_loss: 0.8970 - emotion_output_loss: 0.8985 - gender_output_acc: 0.6150 - image_quality_output_acc: 0.5586 - age_output_acc: 0.3972 - weight_output_acc: 0.6394 - bag_output_acc: 0.5636 - footwear_output_acc: 0.5879 - pose_output_acc: 0.6185 - emotion_output_acc: 0.7119 - val_loss: 7.6303 - val_gender_output_loss: 0.6373 - val_image_quality_output_loss: 0.9939 - val_age_output_loss: 1.4003 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.9139 - val_footwear_output_loss: 0.8811 - val_pose_output_loss: 0.9020 - val_emotion_output_loss: 0.9082 - val_gender_output_acc: 0.6230 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.4062 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5847 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 46/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.5921 - gender_output_loss: 0.6519 - image_quality_output_loss: 0.9635 - age_output_loss: 1.4016 - weight_output_loss: 0.9680 - bag_output_loss: 0.9062 - footwear_output_loss: 0.8999 - pose_output_loss: 0.9016 - emotion_output_loss: 0.8994 - gender_output_acc: 0.6028 - image_quality_output_acc: 0.5579 - age_output_acc: 0.3994 - weight_output_acc: 0.6392 - bag_output_acc: 0.5631 - footwear_output_acc: 0.5878 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7121 - val_loss: 7.6565 - val_gender_output_loss: 0.6390 - val_image_quality_output_loss: 0.9995 - val_age_output_loss: 1.4144 - val_weight_output_loss: 1.0023 - val_bag_output_loss: 0.9076 - val_footwear_output_loss: 0.8732 - val_pose_output_loss: 0.9091 - val_emotion_output_loss: 0.9113 - val_gender_output_acc: 0.6285 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6089 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 47/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.5477 - gender_output_loss: 0.6369 - image_quality_output_loss: 0.9619 - age_output_loss: 1.3953 - weight_output_loss: 0.9665 - bag_output_loss: 0.9043 - footwear_output_loss: 0.8857 - pose_output_loss: 0.8979 - emotion_output_loss: 0.8991 - gender_output_acc: 0.6209 - image_quality_output_acc: 0.5580 - age_output_acc: 0.3986 - weight_output_acc: 0.6393 - bag_output_acc: 0.5639 - footwear_output_acc: 0.5970 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7118 - val_loss: 7.6243 - val_gender_output_loss: 0.6331 - val_image_quality_output_loss: 0.9998 - val_age_output_loss: 1.4101 - val_weight_output_loss: 0.9998 - val_bag_output_loss: 0.9074 - val_footwear_output_loss: 0.8605 - val_pose_output_loss: 0.9029 - val_emotion_output_loss: 0.9108 - val_gender_output_acc: 0.6351 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6164 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 48/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5519 - gender_output_loss: 0.6352 - image_quality_output_loss: 0.9639 - age_output_loss: 1.3999 - weight_output_loss: 0.9684 - bag_output_loss: 0.9041 - footwear_output_loss: 0.8835 - pose_output_loss: 0.8989 - emotion_output_loss: 0.8979 - gender_output_acc: 0.6224 - image_quality_output_acc: 0.5592 - age_output_acc: 0.4002 - weight_output_acc: 0.6394 - bag_output_acc: 0.5639 - footwear_output_acc: 0.5990 - pose_output_acc: 0.6191 - emotion_output_acc: 0.7122 - val_loss: 7.5940 - val_gender_output_loss: 0.6245 - val_image_quality_output_loss: 0.9969 - val_age_output_loss: 1.4050 - val_weight_output_loss: 0.9966 - val_bag_output_loss: 0.9052 - val_footwear_output_loss: 0.8572 - val_pose_output_loss: 0.8987 - val_emotion_output_loss: 0.9098 - val_gender_output_acc: 0.6366 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.3972 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6174 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 49/100\n",
            "180/180 [==============================] - 91s 507ms/step - loss: 7.6089 - gender_output_loss: 0.6706 - image_quality_output_loss: 0.9650 - age_output_loss: 1.4075 - weight_output_loss: 0.9682 - bag_output_loss: 0.9127 - footwear_output_loss: 0.8882 - pose_output_loss: 0.8975 - emotion_output_loss: 0.8993 - gender_output_acc: 0.5881 - image_quality_output_acc: 0.5592 - age_output_acc: 0.3963 - weight_output_acc: 0.6396 - bag_output_acc: 0.5635 - footwear_output_acc: 0.6018 - pose_output_acc: 0.6196 - emotion_output_acc: 0.7122 - val_loss: 7.6416 - val_gender_output_loss: 0.6500 - val_image_quality_output_loss: 0.9979 - val_age_output_loss: 1.4081 - val_weight_output_loss: 0.9998 - val_bag_output_loss: 0.9147 - val_footwear_output_loss: 0.8638 - val_pose_output_loss: 0.8933 - val_emotion_output_loss: 0.9139 - val_gender_output_acc: 0.6124 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6053 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 50/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6100 - gender_output_loss: 0.6668 - image_quality_output_loss: 0.9641 - age_output_loss: 1.4096 - weight_output_loss: 0.9682 - bag_output_loss: 0.9115 - footwear_output_loss: 0.8813 - pose_output_loss: 0.9078 - emotion_output_loss: 0.9006 - gender_output_acc: 0.5898 - image_quality_output_acc: 0.5564 - age_output_acc: 0.3982 - weight_output_acc: 0.6394 - bag_output_acc: 0.5634 - footwear_output_acc: 0.6029 - pose_output_acc: 0.6194 - emotion_output_acc: 0.7122 - val_loss: 7.7409 - val_gender_output_loss: 0.6872 - val_image_quality_output_loss: 0.9901 - val_age_output_loss: 1.4367 - val_weight_output_loss: 1.0029 - val_bag_output_loss: 0.9164 - val_footwear_output_loss: 0.8755 - val_pose_output_loss: 0.9158 - val_emotion_output_loss: 0.9161 - val_gender_output_acc: 0.5403 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6053 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 51/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6036 - gender_output_loss: 0.6618 - image_quality_output_loss: 0.9625 - age_output_loss: 1.4104 - weight_output_loss: 0.9678 - bag_output_loss: 0.9097 - footwear_output_loss: 0.8838 - pose_output_loss: 0.9067 - emotion_output_loss: 0.9009 - gender_output_acc: 0.5986 - image_quality_output_acc: 0.5575 - age_output_acc: 0.3981 - weight_output_acc: 0.6394 - bag_output_acc: 0.5639 - footwear_output_acc: 0.5974 - pose_output_acc: 0.6184 - emotion_output_acc: 0.7120 - val_loss: 7.6423 - val_gender_output_loss: 0.6523 - val_image_quality_output_loss: 0.9898 - val_age_output_loss: 1.4108 - val_weight_output_loss: 0.9945 - val_bag_output_loss: 0.9122 - val_footwear_output_loss: 0.8599 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.9146 - val_gender_output_acc: 0.6220 - val_image_quality_output_acc: 0.5318 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5685 - val_footwear_output_acc: 0.6023 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 52/100\n",
            "180/180 [==============================] - 91s 508ms/step - loss: 7.6021 - gender_output_loss: 0.6638 - image_quality_output_loss: 0.9613 - age_output_loss: 1.4069 - weight_output_loss: 0.9683 - bag_output_loss: 0.9091 - footwear_output_loss: 0.8866 - pose_output_loss: 0.9054 - emotion_output_loss: 0.9007 - gender_output_acc: 0.5956 - image_quality_output_acc: 0.5579 - age_output_acc: 0.3980 - weight_output_acc: 0.6394 - bag_output_acc: 0.5643 - footwear_output_acc: 0.5958 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7120 - val_loss: 7.6613 - val_gender_output_loss: 0.6490 - val_image_quality_output_loss: 0.9917 - val_age_output_loss: 1.4105 - val_weight_output_loss: 0.9971 - val_bag_output_loss: 0.9164 - val_footwear_output_loss: 0.8776 - val_pose_output_loss: 0.9079 - val_emotion_output_loss: 0.9110 - val_gender_output_acc: 0.6129 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5993 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 53/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5861 - gender_output_loss: 0.6547 - image_quality_output_loss: 0.9614 - age_output_loss: 1.4065 - weight_output_loss: 0.9684 - bag_output_loss: 0.9089 - footwear_output_loss: 0.8826 - pose_output_loss: 0.9041 - emotion_output_loss: 0.8995 - gender_output_acc: 0.6168 - image_quality_output_acc: 0.5598 - age_output_acc: 0.3983 - weight_output_acc: 0.6398 - bag_output_acc: 0.5653 - footwear_output_acc: 0.6040 - pose_output_acc: 0.6178 - emotion_output_acc: 0.7122 - val_loss: 7.6675 - val_gender_output_loss: 0.6516 - val_image_quality_output_loss: 0.9983 - val_age_output_loss: 1.4187 - val_weight_output_loss: 0.9984 - val_bag_output_loss: 0.9127 - val_footwear_output_loss: 0.8602 - val_pose_output_loss: 0.9095 - val_emotion_output_loss: 0.9181 - val_gender_output_acc: 0.6280 - val_image_quality_output_acc: 0.5242 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5675 - val_footwear_output_acc: 0.6144 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 54/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6406 - gender_output_loss: 0.6671 - image_quality_output_loss: 0.9683 - age_output_loss: 1.4157 - weight_output_loss: 0.9717 - bag_output_loss: 0.9096 - footwear_output_loss: 0.8934 - pose_output_loss: 0.9129 - emotion_output_loss: 0.9019 - gender_output_acc: 0.5856 - image_quality_output_acc: 0.5581 - age_output_acc: 0.3968 - weight_output_acc: 0.6392 - bag_output_acc: 0.5644 - footwear_output_acc: 0.5925 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7123 - val_loss: 7.7010 - val_gender_output_loss: 0.6497 - val_image_quality_output_loss: 1.0014 - val_age_output_loss: 1.4191 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.9153 - val_footwear_output_loss: 0.8801 - val_pose_output_loss: 0.9264 - val_emotion_output_loss: 0.9118 - val_gender_output_acc: 0.5958 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5857 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 55/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6240 - gender_output_loss: 0.6625 - image_quality_output_loss: 0.9687 - age_output_loss: 1.4143 - weight_output_loss: 0.9717 - bag_output_loss: 0.9095 - footwear_output_loss: 0.8854 - pose_output_loss: 0.9113 - emotion_output_loss: 0.9004 - gender_output_acc: 0.5994 - image_quality_output_acc: 0.5586 - age_output_acc: 0.3978 - weight_output_acc: 0.6393 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5970 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7120 - val_loss: 7.6680 - val_gender_output_loss: 0.6406 - val_image_quality_output_loss: 1.0018 - val_age_output_loss: 1.4163 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.9131 - val_footwear_output_loss: 0.8600 - val_pose_output_loss: 0.9230 - val_emotion_output_loss: 0.9160 - val_gender_output_acc: 0.6366 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3947 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6149 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 56/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6171 - gender_output_loss: 0.6630 - image_quality_output_loss: 0.9655 - age_output_loss: 1.4130 - weight_output_loss: 0.9697 - bag_output_loss: 0.9105 - footwear_output_loss: 0.8887 - pose_output_loss: 0.9066 - emotion_output_loss: 0.9003 - gender_output_acc: 0.5990 - image_quality_output_acc: 0.5571 - age_output_acc: 0.3963 - weight_output_acc: 0.6394 - bag_output_acc: 0.5638 - footwear_output_acc: 0.6008 - pose_output_acc: 0.6192 - emotion_output_acc: 0.7121 - val_loss: 7.6563 - val_gender_output_loss: 0.6471 - val_image_quality_output_loss: 0.9997 - val_age_output_loss: 1.4155 - val_weight_output_loss: 0.9956 - val_bag_output_loss: 0.9134 - val_footwear_output_loss: 0.8620 - val_pose_output_loss: 0.9116 - val_emotion_output_loss: 0.9114 - val_gender_output_acc: 0.6381 - val_image_quality_output_acc: 0.5272 - val_age_output_acc: 0.3987 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6159 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 57/100\n",
            "180/180 [==============================] - 91s 507ms/step - loss: 7.5827 - gender_output_loss: 0.6549 - image_quality_output_loss: 0.9655 - age_output_loss: 1.4100 - weight_output_loss: 0.9685 - bag_output_loss: 0.9075 - footwear_output_loss: 0.8741 - pose_output_loss: 0.9028 - emotion_output_loss: 0.8994 - gender_output_acc: 0.6095 - image_quality_output_acc: 0.5578 - age_output_acc: 0.3990 - weight_output_acc: 0.6396 - bag_output_acc: 0.5637 - footwear_output_acc: 0.6062 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7118 - val_loss: 7.7010 - val_gender_output_loss: 0.6622 - val_image_quality_output_loss: 0.9968 - val_age_output_loss: 1.4111 - val_weight_output_loss: 0.9944 - val_bag_output_loss: 0.9125 - val_footwear_output_loss: 0.8837 - val_pose_output_loss: 0.9293 - val_emotion_output_loss: 0.9111 - val_gender_output_acc: 0.5806 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.4012 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5973 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 58/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5793 - gender_output_loss: 0.6492 - image_quality_output_loss: 0.9633 - age_output_loss: 1.4087 - weight_output_loss: 0.9689 - bag_output_loss: 0.9076 - footwear_output_loss: 0.8782 - pose_output_loss: 0.9032 - emotion_output_loss: 0.9003 - gender_output_acc: 0.6193 - image_quality_output_acc: 0.5572 - age_output_acc: 0.3982 - weight_output_acc: 0.6391 - bag_output_acc: 0.5640 - footwear_output_acc: 0.6028 - pose_output_acc: 0.6189 - emotion_output_acc: 0.7120 - val_loss: 7.6289 - val_gender_output_loss: 0.6440 - val_image_quality_output_loss: 1.0004 - val_age_output_loss: 1.4060 - val_weight_output_loss: 0.9976 - val_bag_output_loss: 0.9138 - val_footwear_output_loss: 0.8509 - val_pose_output_loss: 0.9063 - val_emotion_output_loss: 0.9099 - val_gender_output_acc: 0.6104 - val_image_quality_output_acc: 0.5282 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6124 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 59/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5663 - gender_output_loss: 0.6485 - image_quality_output_loss: 0.9635 - age_output_loss: 1.4043 - weight_output_loss: 0.9692 - bag_output_loss: 0.9060 - footwear_output_loss: 0.8710 - pose_output_loss: 0.9039 - emotion_output_loss: 0.8999 - gender_output_acc: 0.6187 - image_quality_output_acc: 0.5566 - age_output_acc: 0.3972 - weight_output_acc: 0.6391 - bag_output_acc: 0.5641 - footwear_output_acc: 0.6061 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7119 - val_loss: 7.6807 - val_gender_output_loss: 0.6427 - val_image_quality_output_loss: 1.0004 - val_age_output_loss: 1.4125 - val_weight_output_loss: 1.0007 - val_bag_output_loss: 0.9132 - val_footwear_output_loss: 0.8893 - val_pose_output_loss: 0.9102 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.6139 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5973 - val_pose_output_acc: 0.6109 - val_emotion_output_acc: 0.7092\n",
            "Epoch 60/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6100 - gender_output_loss: 0.6606 - image_quality_output_loss: 0.9648 - age_output_loss: 1.4046 - weight_output_loss: 0.9697 - bag_output_loss: 0.9057 - footwear_output_loss: 0.8965 - pose_output_loss: 0.9089 - emotion_output_loss: 0.8993 - gender_output_acc: 0.5945 - image_quality_output_acc: 0.5592 - age_output_acc: 0.3985 - weight_output_acc: 0.6393 - bag_output_acc: 0.5631 - footwear_output_acc: 0.5910 - pose_output_acc: 0.6190 - emotion_output_acc: 0.7122 - val_loss: 7.7301 - val_gender_output_loss: 0.6657 - val_image_quality_output_loss: 0.9989 - val_age_output_loss: 1.4166 - val_weight_output_loss: 1.0015 - val_bag_output_loss: 0.9135 - val_footwear_output_loss: 0.9072 - val_pose_output_loss: 0.9133 - val_emotion_output_loss: 0.9134 - val_gender_output_acc: 0.5882 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5817 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 61/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.5967 - gender_output_loss: 0.6549 - image_quality_output_loss: 0.9633 - age_output_loss: 1.4052 - weight_output_loss: 0.9678 - bag_output_loss: 0.9075 - footwear_output_loss: 0.8891 - pose_output_loss: 0.9087 - emotion_output_loss: 0.9002 - gender_output_acc: 0.6037 - image_quality_output_acc: 0.5571 - age_output_acc: 0.4008 - weight_output_acc: 0.6396 - bag_output_acc: 0.5634 - footwear_output_acc: 0.5936 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7117 - val_loss: 7.6278 - val_gender_output_loss: 0.6353 - val_image_quality_output_loss: 0.9998 - val_age_output_loss: 1.4072 - val_weight_output_loss: 0.9960 - val_bag_output_loss: 0.9087 - val_footwear_output_loss: 0.8635 - val_pose_output_loss: 0.9046 - val_emotion_output_loss: 0.9126 - val_gender_output_acc: 0.6462 - val_image_quality_output_acc: 0.5302 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6139 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 62/100\n",
            "180/180 [==============================] - 92s 511ms/step - loss: 7.5496 - gender_output_loss: 0.6410 - image_quality_output_loss: 0.9634 - age_output_loss: 1.4032 - weight_output_loss: 0.9658 - bag_output_loss: 0.9051 - footwear_output_loss: 0.8722 - pose_output_loss: 0.8997 - emotion_output_loss: 0.8992 - gender_output_acc: 0.6233 - image_quality_output_acc: 0.5577 - age_output_acc: 0.4007 - weight_output_acc: 0.6395 - bag_output_acc: 0.5637 - footwear_output_acc: 0.6072 - pose_output_acc: 0.6186 - emotion_output_acc: 0.7116 - val_loss: 7.5966 - val_gender_output_loss: 0.6253 - val_image_quality_output_loss: 0.9987 - val_age_output_loss: 1.4029 - val_weight_output_loss: 0.9913 - val_bag_output_loss: 0.9089 - val_footwear_output_loss: 0.8581 - val_pose_output_loss: 0.9000 - val_emotion_output_loss: 0.9115 - val_gender_output_acc: 0.6507 - val_image_quality_output_acc: 0.5312 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6119 - val_pose_output_acc: 0.6124 - val_emotion_output_acc: 0.7092\n",
            "Epoch 63/100\n",
            "180/180 [==============================] - 92s 511ms/step - loss: 7.5950 - gender_output_loss: 0.6587 - image_quality_output_loss: 0.9623 - age_output_loss: 1.4015 - weight_output_loss: 0.9649 - bag_output_loss: 0.9046 - footwear_output_loss: 0.9045 - pose_output_loss: 0.9005 - emotion_output_loss: 0.8981 - gender_output_acc: 0.5907 - image_quality_output_acc: 0.5569 - age_output_acc: 0.4010 - weight_output_acc: 0.6398 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5843 - pose_output_acc: 0.6188 - emotion_output_acc: 0.7125 - val_loss: 7.6838 - val_gender_output_loss: 0.6585 - val_image_quality_output_loss: 0.9993 - val_age_output_loss: 1.4130 - val_weight_output_loss: 0.9955 - val_bag_output_loss: 0.9107 - val_footwear_output_loss: 0.8870 - val_pose_output_loss: 0.9057 - val_emotion_output_loss: 0.9141 - val_gender_output_acc: 0.5877 - val_image_quality_output_acc: 0.5287 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5927 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 64/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6072 - gender_output_loss: 0.6591 - image_quality_output_loss: 0.9633 - age_output_loss: 1.4025 - weight_output_loss: 0.9676 - bag_output_loss: 0.9055 - footwear_output_loss: 0.9070 - pose_output_loss: 0.9035 - emotion_output_loss: 0.8987 - gender_output_acc: 0.5830 - image_quality_output_acc: 0.5584 - age_output_acc: 0.3989 - weight_output_acc: 0.6394 - bag_output_acc: 0.5635 - footwear_output_acc: 0.5799 - pose_output_acc: 0.6181 - emotion_output_acc: 0.7120 - val_loss: 7.6817 - val_gender_output_loss: 0.6513 - val_image_quality_output_loss: 1.0013 - val_age_output_loss: 1.4079 - val_weight_output_loss: 0.9973 - val_bag_output_loss: 0.9102 - val_footwear_output_loss: 0.8964 - val_pose_output_loss: 0.9057 - val_emotion_output_loss: 0.9117 - val_gender_output_acc: 0.6033 - val_image_quality_output_acc: 0.5282 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5852 - val_pose_output_acc: 0.6134 - val_emotion_output_acc: 0.7092\n",
            "Epoch 65/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5913 - gender_output_loss: 0.6574 - image_quality_output_loss: 0.9618 - age_output_loss: 1.4025 - weight_output_loss: 0.9689 - bag_output_loss: 0.9041 - footwear_output_loss: 0.8974 - pose_output_loss: 0.9006 - emotion_output_loss: 0.8985 - gender_output_acc: 0.5984 - image_quality_output_acc: 0.5579 - age_output_acc: 0.3995 - weight_output_acc: 0.6393 - bag_output_acc: 0.5637 - footwear_output_acc: 0.5892 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7120 - val_loss: 7.6679 - val_gender_output_loss: 0.6499 - val_image_quality_output_loss: 0.9964 - val_age_output_loss: 1.4064 - val_weight_output_loss: 1.0030 - val_bag_output_loss: 0.9086 - val_footwear_output_loss: 0.8912 - val_pose_output_loss: 0.9001 - val_emotion_output_loss: 0.9124 - val_gender_output_acc: 0.6033 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5968 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 66/100\n",
            "180/180 [==============================] - 92s 510ms/step - loss: 7.6044 - gender_output_loss: 0.6599 - image_quality_output_loss: 0.9621 - age_output_loss: 1.4044 - weight_output_loss: 0.9681 - bag_output_loss: 0.9091 - footwear_output_loss: 0.9032 - pose_output_loss: 0.8985 - emotion_output_loss: 0.8992 - gender_output_acc: 0.5908 - image_quality_output_acc: 0.5576 - age_output_acc: 0.3973 - weight_output_acc: 0.6395 - bag_output_acc: 0.5633 - footwear_output_acc: 0.5878 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7120 - val_loss: 7.7247 - val_gender_output_loss: 0.6566 - val_image_quality_output_loss: 1.0077 - val_age_output_loss: 1.4080 - val_weight_output_loss: 1.0032 - val_bag_output_loss: 0.9181 - val_footwear_output_loss: 0.9002 - val_pose_output_loss: 0.9158 - val_emotion_output_loss: 0.9152 - val_gender_output_acc: 0.5953 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.5887 - val_pose_output_acc: 0.6164 - val_emotion_output_acc: 0.7092\n",
            "Epoch 67/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.6281 - gender_output_loss: 0.6663 - image_quality_output_loss: 0.9614 - age_output_loss: 1.4065 - weight_output_loss: 0.9674 - bag_output_loss: 0.9119 - footwear_output_loss: 0.9115 - pose_output_loss: 0.9027 - emotion_output_loss: 0.9004 - gender_output_acc: 0.5918 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3967 - weight_output_acc: 0.6392 - bag_output_acc: 0.5639 - footwear_output_acc: 0.5847 - pose_output_acc: 0.6201 - emotion_output_acc: 0.7119 - val_loss: 7.6716 - val_gender_output_loss: 0.6515 - val_image_quality_output_loss: 1.0001 - val_age_output_loss: 1.4052 - val_weight_output_loss: 0.9983 - val_bag_output_loss: 0.9150 - val_footwear_output_loss: 0.8879 - val_pose_output_loss: 0.9016 - val_emotion_output_loss: 0.9120 - val_gender_output_acc: 0.5988 - val_image_quality_output_acc: 0.5277 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6028 - val_pose_output_acc: 0.6119 - val_emotion_output_acc: 0.7092\n",
            "Epoch 68/100\n",
            "180/180 [==============================] - 92s 511ms/step - loss: 7.6106 - gender_output_loss: 0.6621 - image_quality_output_loss: 0.9617 - age_output_loss: 1.4033 - weight_output_loss: 0.9679 - bag_output_loss: 0.9111 - footwear_output_loss: 0.9060 - pose_output_loss: 0.8998 - emotion_output_loss: 0.8988 - gender_output_acc: 0.5924 - image_quality_output_acc: 0.5583 - age_output_acc: 0.3980 - weight_output_acc: 0.6394 - bag_output_acc: 0.5640 - footwear_output_acc: 0.5852 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7123 - val_loss: 7.6535 - val_gender_output_loss: 0.6469 - val_image_quality_output_loss: 0.9905 - val_age_output_loss: 1.4080 - val_weight_output_loss: 0.9968 - val_bag_output_loss: 0.9148 - val_footwear_output_loss: 0.8740 - val_pose_output_loss: 0.9083 - val_emotion_output_loss: 0.9143 - val_gender_output_acc: 0.6174 - val_image_quality_output_acc: 0.5297 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5665 - val_footwear_output_acc: 0.6058 - val_pose_output_acc: 0.6144 - val_emotion_output_acc: 0.7092\n",
            "Epoch 69/100\n",
            "180/180 [==============================] - 92s 509ms/step - loss: 7.5974 - gender_output_loss: 0.6561 - image_quality_output_loss: 0.9583 - age_output_loss: 1.3997 - weight_output_loss: 0.9647 - bag_output_loss: 0.9109 - footwear_output_loss: 0.9056 - pose_output_loss: 0.9009 - emotion_output_loss: 0.9011 - gender_output_acc: 0.6049 - image_quality_output_acc: 0.5575 - age_output_acc: 0.3981 - weight_output_acc: 0.6393 - bag_output_acc: 0.5643 - footwear_output_acc: 0.5868 - pose_output_acc: 0.6182 - emotion_output_acc: 0.7119 - val_loss: 7.6871 - val_gender_output_loss: 0.6526 - val_image_quality_output_loss: 0.9900 - val_age_output_loss: 1.4135 - val_weight_output_loss: 1.0057 - val_bag_output_loss: 0.9156 - val_footwear_output_loss: 0.8926 - val_pose_output_loss: 0.9044 - val_emotion_output_loss: 0.9127 - val_gender_output_acc: 0.6134 - val_image_quality_output_acc: 0.5292 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5675 - val_footwear_output_acc: 0.5983 - val_pose_output_acc: 0.6149 - val_emotion_output_acc: 0.7092\n",
            "Epoch 70/100\n",
            "180/180 [==============================] - 92s 511ms/step - loss: 7.5769 - gender_output_loss: 0.6508 - image_quality_output_loss: 0.9527 - age_output_loss: 1.3993 - weight_output_loss: 0.9645 - bag_output_loss: 0.9099 - footwear_output_loss: 0.9016 - pose_output_loss: 0.8980 - emotion_output_loss: 0.9001 - gender_output_acc: 0.6089 - image_quality_output_acc: 0.5591 - age_output_acc: 0.3988 - weight_output_acc: 0.6394 - bag_output_acc: 0.5648 - footwear_output_acc: 0.5860 - pose_output_acc: 0.6193 - emotion_output_acc: 0.7118 - val_loss: 7.6213 - val_gender_output_loss: 0.6388 - val_image_quality_output_loss: 0.9960 - val_age_output_loss: 1.3982 - val_weight_output_loss: 0.9959 - val_bag_output_loss: 0.9099 - val_footwear_output_loss: 0.8701 - val_pose_output_loss: 0.9024 - val_emotion_output_loss: 0.9101 - val_gender_output_acc: 0.6195 - val_image_quality_output_acc: 0.5328 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5670 - val_footwear_output_acc: 0.6079 - val_pose_output_acc: 0.6114 - val_emotion_output_acc: 0.7092\n",
            "Epoch 71/100\n",
            "  4/180 [..............................] - ETA: 1:33 - loss: 7.5360 - gender_output_loss: 0.6297 - image_quality_output_loss: 0.9015 - age_output_loss: 1.3837 - weight_output_loss: 0.9657 - bag_output_loss: 0.8605 - footwear_output_loss: 0.9455 - pose_output_loss: 0.9989 - emotion_output_loss: 0.8505 - gender_output_acc: 0.6523 - image_quality_output_acc: 0.6055 - age_output_acc: 0.3984 - weight_output_acc: 0.6289 - bag_output_acc: 0.5664 - footwear_output_acc: 0.5742 - pose_output_acc: 0.5391 - emotion_output_acc: 0.7305"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-998:\n",
            "Process ForkPoolWorker-995:\n",
            "Process ForkPoolWorker-993:\n",
            "Process ForkPoolWorker-991:\n",
            "Process ForkPoolWorker-1001:\n",
            "Process ForkPoolWorker-997:\n",
            "Process ForkPoolWorker-999:\n",
            "Process ForkPoolWorker-996:\n",
            "Process ForkPoolWorker-994:\n",
            "Process ForkPoolWorker-1000:\n",
            "Process ForkPoolWorker-1002:\n",
            "Process ForkPoolWorker-992:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-d8ce8af559f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}